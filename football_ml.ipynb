{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo\n",
    "- metric\n",
    "- cross evaluation\n",
    "- hyper parameter grid search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn import model_selection\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from mypkg.football_ml_dataset import Feature\n",
    "from mypkg.football_ml_dataset import MatchStatsDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "The data is loaded into MatchStatsDataset objects with data from one season in each. The first 30 matches in each season are dropped because some history is needed to make predictions. Results from last season is considered inadequate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read csv and initialize MatchStatsDataset objects...\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 48 fields in line 513, saw 49\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-215c366eec21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Read csv and initialize MatchStatsDataset objects...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMatchStatsDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Appl\\anaconda3\\envs\\olind37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Appl\\anaconda3\\envs\\olind37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Appl\\anaconda3\\envs\\olind37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nrows'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Appl\\anaconda3\\envs\\olind37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1993\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1995\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1996\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 48 fields in line 513, saw 49\n"
     ]
    }
   ],
   "source": [
    "path ='C:/Users/olind/ML/'\n",
    "\n",
    "files = ['PL0001.csv', 'PL0102.csv', 'PL0203.csv', 'PL0304.csv', 'PL0405.csv',\n",
    "         'PL0506.csv', 'PL0607.csv', 'PL0708.csv', 'PL0809.csv', 'PL0910.csv',\n",
    "         'PL1011.csv', 'PL1112.csv', 'PL1213.csv','PL1314.csv', 'PL1415.csv',\n",
    "         'PL1516.csv', 'PL1617.csv', 'PL1718.csv']\n",
    "datasets = []\n",
    "print('Read csv and initialize MatchStatsDataset objects...')\n",
    "for file in files:\n",
    "    df = pd.read_csv(path+file)\n",
    "    datasets.append(MatchStatsDataset(df, 30))\n",
    "    \n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc features\n",
    "Features are calculated from past games.\n",
    "The dictionary dct_features is creating Feature objects (from football_ml_dataset), that defines which features to calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_features = {'ht_prcwin_prev5_homegames':Feature('H', 'prcwin', 5, 'H'),\n",
    "            'at_prcwin_prev5_awaygames':Feature('A', 'prcwin', 5, 'A'),\n",
    "            #'ht_goalsscored_prev5_homegames':Feature('H', 'goalsscored', 5, 'H'),\n",
    "            #'at_goalsscored_prev5_awaygames':Feature('A', 'goalsscored', 5, 'A'),\n",
    "            'ht_goalsagainst_prev5_homegames':Feature('H', 'goalsagainst', 5, 'H'),\n",
    "            'at_goalsagainst_prev5_awaygames':Feature('A', 'goalsagainst',5, 'A'),\n",
    "            'ht_goalsscoredewma_prev20_homegames':Feature('H', 'goalsscoredewma', 20, 'H'),\n",
    "            'at_goalsscoredewma_prev20_awaygames':Feature('A', 'goalsscoredewma', 20, 'A'),\n",
    "            }\n",
    "\n",
    "#Calc features and aggregate match data into one dataframe\n",
    "#The final dataframe contains both features and other match data (like FTR)\n",
    "print('Calculating features ...')\n",
    "df = pd.DataFrame()\n",
    "for i, ds in enumerate(datasets):\n",
    "    dftmp = ds.get_features_and_matchdata(dct_features)\n",
    "    df = dftmp if df.empty else df.append(dftmp)\n",
    "\n",
    "#Do various postprocessing\n",
    "df.index = range(df.shape[0])\n",
    "for col in dct_features:\n",
    "    df[col] = pd.to_numeric(df[col])\n",
    "df = df[df.FTR.notnull()]\n",
    "df = df.dropna()\n",
    "\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data\n",
    "We are looking for feature differences for the result categories. That would make the feature a potential useful variable in the machine learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for feature in dct_features:\n",
    "    #ax = sns.distplot(df[feature])\n",
    "    #plt.show()\n",
    "    g = sns.catplot(x='FTR', y=feature, hue='FTR', data=df, kind=\"box\", row_order=['H', 'D', 'A'])\n",
    "    g.fig.suptitle(feature) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is a lot of overlap between the categories and since I am more interested in the probabilities than the actual predicted classification, I will make random samples from the dataset and plot the features against the probability of each outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(dct_features.keys())  + ['prc_home', 'prc_draw', 'prc_away']\n",
    "df_samples = pd.DataFrame(columns = columns, dtype = float)\n",
    "\n",
    "for i in range(100):\n",
    "    sample = df.sample(n=15, random_state=i)\n",
    "    df_samples.loc[i,:] = sample.mean()\n",
    "    FTR_count = sample.FTR.value_counts()\n",
    "    df_samples.loc[i,'prc_home'] = (FTR_count['H']/sample.shape[0]) if 'H' in FTR_count else 0\n",
    "    df_samples.loc[i,'prc_draw'] = (FTR_count['D']/sample.shape[0]) if 'D' in FTR_count else 0\n",
    "    df_samples.loc[i,'prc_away'] = (FTR_count['A']/sample.shape[0]) if 'A' in FTR_count else 0\n",
    "    \n",
    "def r2(x, y):\n",
    "    return stats.pearsonr(x, y)[0] ** 2\n",
    "\n",
    "for col in dct_features:\n",
    "    #df_samples.plot.scatter(x=col, y='prc_home')\n",
    "    sns.jointplot(data=df_samples, x=col, y='prc_home', stat_func=r2)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data: split, transform and scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get features and labels as numpy\n",
    "features = df[dct_features.keys()].to_numpy()\n",
    "labels = df.FTR.replace({}).replace({'H':0, 'D':1, 'A':2}).to_numpy()\n",
    "\n",
    "#Split\n",
    "indx = range(features.shape[0])\n",
    "test_size = int(features.shape[0]/10)\n",
    "indx = model_selection.train_test_split(indx, test_size=test_size)\n",
    "x_train = features[indx[0],:]\n",
    "y_train = labels[indx[0]]\n",
    "x_test = features[indx[1]]\n",
    "y_test = labels[indx[1]]\n",
    "\n",
    "#Transform\n",
    "pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "pt.fit_transform(x_train)\n",
    "x_train = pt.transform(x_train)\n",
    "x_test = pt.transform(x_test)\n",
    "\n",
    "#Scale\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot distributions of transformed and scaled data\n",
    "df_processed = pd.DataFrame(x_train, columns=list(dct_features.keys()))\n",
    "for col in df_processed:\n",
    "    sns.distplot(df_processed[col], bins=8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fitting Models ...')\n",
    "\n",
    "model_logreg = LogisticRegression(random_state=42, solver='lbfgs', multi_class='multinomial')\n",
    "model_rfc = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0)\n",
    "model_logreg.fit(x_train, y_train)\n",
    "model_rfc.fit(x_train, y_train)\n",
    "\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pred_quality_plot(match_outcome, model, bins=10):\n",
    "\n",
    "    model_probs = pd.DataFrame(model.predict_proba(x_test), columns = ['p_home', 'p_draw', 'p_away'])\n",
    "    bookie_probs = df.iloc[indx[1],:]\n",
    "    bookie_probs.index = range(bookie_probs.shape[0])\n",
    "    model_probs['FTR'] = bookie_probs['FTR']\n",
    "    \n",
    "    p_bins = [x/bins for x in range(bins)]\n",
    "    \n",
    "    keymap = {'H':'p_home', 'D':'p_draw', 'A':'p_away'}\n",
    "    key = keymap[match_outcome]\n",
    "    \n",
    "    est_bookie, est_model, act_bookie, act_model = [], [], [], []\n",
    "    \n",
    "    for p in p_bins:\n",
    "        #bookmaker\n",
    "        df_bookie = bookie_probs[bookie_probs[key] >= p]\n",
    "        df_bookie = df_bookie[df_bookie[key] < p+1/bins]\n",
    "        #model\n",
    "        df_model = model_probs[model_probs[key] >= p]\n",
    "        df_model = df_model[df_model[key] < p+1/bins]\n",
    "            \n",
    "        count_act_b = df_bookie.FTR.value_counts()\n",
    "        count_act_m = df_model.FTR.value_counts()\n",
    "            \n",
    "        est_bookie.append(df_bookie[key].mean())\n",
    "        est_model.append(df_model[key].mean())\n",
    "        \n",
    "        val = count_act_b[match_outcome]/df_bookie.shape[0] if match_outcome in count_act_b.index else 0\n",
    "        act_bookie.append(val)\n",
    "        \n",
    "        val = count_act_m[match_outcome]/df_model.shape[0] if match_outcome in count_act_m.index else 0\n",
    "        act_model.append(val)\n",
    "        \n",
    "    plt.scatter(est_bookie, act_bookie, color='blue', label='bookie')\n",
    "    plt.scatter(est_model, act_model, color='red', label='model, R2')\n",
    "    plt.plot([0,1], [0,1], label='Perfect match', color='black')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Test data set: Model vs bookie for match outcomes: '+match_outcome)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "pred_quality_plot('H', model_logreg)\n",
    "pred_quality_plot('D', model_logreg)\n",
    "pred_quality_plot('A', model_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trading strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def bet(homedrawaway, amount, odds, ftr):\n",
    "    if homedrawaway == ftr:\n",
    "        return amount*(odds-1)\n",
    "    else:\n",
    "        return -amount\n",
    "\n",
    "def trading_strategy(model):\n",
    "\n",
    "    pnl, total_inv = 0, 0\n",
    "    pnls, return_rate = [0], [0]\n",
    "    \n",
    "    model_probs = pd.DataFrame(model.predict_proba(x_test), columns = ['p_home', 'p_draw', 'p_away'])\n",
    "    bookie_probs = df.iloc[indx[1],:]\n",
    "    bookie_probs.index = range(bookie_probs.shape[0])\n",
    "\n",
    "    for i in range(model_probs.shape[0]):\n",
    "        p_model_home = model_probs.loc[i, 'p_home']\n",
    "        p_model_draw = model_probs.loc[i, 'p_draw']\n",
    "        p_model_away = model_probs.loc[i, 'p_away']\n",
    "        p_bookie_home = bookie_probs.loc[i, 'p_home']\n",
    "        p_bookie_draw = bookie_probs.loc[i, 'p_draw']\n",
    "        p_bookie_away = bookie_probs.loc[i, 'p_away']\n",
    "        ftr = bookie_probs.loc[i, 'FTR']\n",
    "\n",
    "        adv_home = p_model_home - p_bookie_home\n",
    "        adv_draw = p_model_draw - p_bookie_draw\n",
    "        adv_away = p_model_away - p_bookie_away\n",
    "\n",
    "        stake = 1\n",
    "        total_inv += stake\n",
    "\n",
    "        if adv_home > adv_draw and adv_home > adv_away:\n",
    "            pnl += bet('H', stake,  1/p_bookie_home, ftr)\n",
    "        elif adv_draw > adv_home and adv_draw > adv_away:\n",
    "           pnl += bet('D', stake, 1/p_bookie_draw, ftr)\n",
    "        elif adv_away > adv_home and adv_away > adv_away:\n",
    "            pnl += bet('A', stake, 1/p_bookie_away, ftr)\n",
    "\n",
    "        pnls.append(pnl)\n",
    "        return_rate.append(pnl/total_inv)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(pnls, color='blue', label='Pnl')\n",
    "    ax.set_xlabel('Matches')\n",
    "    ax.set_ylabel('PnL')\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(return_rate, color='red', label='Return')\n",
    "    ax2.set_ylabel('Return')\n",
    "    fig.set_size_inches(10, 8)\n",
    "    ax.legend(loc=0)\n",
    "    ax2.legend(loc=0)\n",
    "    ax2.set_ylim(-0.2, 0.2)\n",
    "    plt.show()\n",
    "\n",
    "trading_strategy(model_logreg)\n",
    "trading_strategy(model_rfc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
